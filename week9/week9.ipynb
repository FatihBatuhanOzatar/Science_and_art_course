{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VS-GzRHfjHgP",
        "outputId": "5465743d-1556-4e22-e0b8-44ee8b37bf4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K\n",
            "changed 22 packages in 646ms\n",
            "\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub streamlit -q\n",
        "!npm install -g localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "# --- SENÄ°N TOKEN'IN ---\n",
        "HF_TOKEN = \"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\"\n",
        "\n",
        "st.set_page_config(page_title=\"GenAI Cloud\", layout=\"wide\")\n",
        "st.title(\"ðŸš€ Science & Art: GenAI (Cloud Version)\")\n",
        "st.success(\"Sistem BaÄŸlandÄ±! / System Connected!\")\n",
        "\n",
        "# Token KontrolÃ¼\n",
        "if not HF_TOKEN.startswith(\"hf_\"):\n",
        "    st.error(\"Token hatasÄ±!\")\n",
        "    st.stop()\n",
        "\n",
        "# Ä°stemci\n",
        "client = InferenceClient(token=HF_TOKEN)\n",
        "\n",
        "# Yan MenÃ¼\n",
        "mode = st.sidebar.radio(\"Select Mode:\", [\"ðŸ’¬ Chat Mode\", \"ðŸŽ¨ Art Mode\"])\n",
        "\n",
        "# --- CHAT MODE ---\n",
        "if mode == \"ðŸ’¬ Chat Mode\":\n",
        "    st.header(\"ðŸ’¬ Chatbot (Mistral 7B)\")\n",
        "    if \"messages\" not in st.session_state: st.session_state.messages = []\n",
        "\n",
        "    for msg in st.session_state.messages:\n",
        "        st.chat_message(msg[\"role\"]).markdown(msg[\"content\"])\n",
        "\n",
        "    if prompt := st.chat_input(\"Ask something...\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        st.chat_message(\"user\").markdown(prompt)\n",
        "\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Thinking...\"):\n",
        "                try:\n",
        "                    response = \"\"\n",
        "                    for token in client.chat_completion([{\"role\": \"user\", \"content\": prompt}], model=\"mistralai/Mistral-7B-Instruct-v0.2\", max_tokens=500, stream=True):\n",
        "                        if token.choices[0].delta.content:\n",
        "                            response += token.choices[0].delta.content\n",
        "                    st.markdown(response)\n",
        "                    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Hata: {e}\")\n",
        "\n",
        "# --- ART MODE ---\n",
        "elif mode == \"ðŸŽ¨ Art Mode\":\n",
        "    st.header(\"ðŸŽ¨ Art Generator (SDXL)\")\n",
        "    prompt = st.text_input(\"Describe image:\", \"A futuristic city\")\n",
        "    if st.button(\"Generate Image\"):\n",
        "        with st.spinner(\"Generating...\"):\n",
        "            try:\n",
        "                image = client.text_to_image(prompt, model=\"stabilityai/stable-diffusion-xl-base-1.0\")\n",
        "                st.image(image, caption=prompt)\n",
        "            except Exception as e:\n",
        "                st.error(f\"Hata: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6oKFyPbjWTR",
        "outputId": "067774c9-070e-4649-b74e-15f9dc0b4b3d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb > cloudflared.deb\n",
        "!dpkg -i cloudflared.deb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utPRrvevjX1T",
        "outputId": "8243b280-ffd5-4463-ab77-2e7cfc8b4c61"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package cloudflared.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack cloudflared.deb ...\n",
            "Unpacking cloudflared (2025.11.1) ...\n",
            "Setting up cloudflared (2025.11.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & cloudflared tunnel --url http://localhost:8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPx4jaRujbLY",
        "outputId": "0b9efabe-1860-4fa0-a3f9-985b46fb8854"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2026-01-10T15:42:17Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2026-01-10T15:42:17Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.126.126.243:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[90m2026-01-10T15:42:21Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2026-01-10T15:42:21Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2026-01-10T15:42:21Z\u001b[0m \u001b[32mINF\u001b[0m |  https://blowing-spencer-suites-rentals.trycloudflare.com                                  |\n",
            "\u001b[90m2026-01-10T15:42:21Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2026-01-10T15:42:21Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2026-01-10T15:42:21Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.11.1 (Checksum 83ea55259e419549817460d0c097f23ad1327364d0a63fab2c5463b9283251cb)\n",
            "\u001b[90m2026-01-10T15:42:21Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.9, GoArch: amd64\n",
            "\u001b[90m2026-01-10T15:42:21Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2026-01-10T15:42:21Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update if installed by a package manager.\n",
            "\u001b[90m2026-01-10T15:42:21Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 03d69c21-3d76-4535-a755-a80ecc64aedd\n",
            "\u001b[90m2026-01-10T15:42:21Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2026-01-10T15:42:21Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2026-01-10T15:42:21Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2026-01-10T15:42:21Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Cannot determine default origin certificate path. No file cert.pem in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]. You need to specify the origin certificate path by specifying the origincert option in the configuration file, or set TUNNEL_ORIGIN_CERT environment variable \u001b[36moriginCertPath=\u001b[0m\n",
            "\u001b[90m2026-01-10T15:42:21Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2026-01-10T15:42:21Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2026-01-10T15:42:21Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2026-01-10T15:42:21Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.7\n",
            "2026/01/10 15:42:21 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2026-01-10T15:42:21Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m7928ef97-e848-4282-80c4-f0ab71384ffb \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.7 \u001b[36mlocation=\u001b[0msin17 \u001b[36mprotocol=\u001b[0mquic\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[90m2026-01-10T15:55:53Z\u001b[0m \u001b[32mINF\u001b[0m Initiating graceful shutdown due to signal interrupt ...\n",
            "\u001b[90m2026-01-10T15:55:53Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to run the datagram handler \u001b[31merror=\u001b[0m\u001b[31m\"context canceled\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.7\n",
            "\u001b[90m2026-01-10T15:55:53Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to serve tunnel connection \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.7\n",
            "\u001b[90m2026-01-10T15:55:53Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Serve tunnel error \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.7\n",
            "\u001b[90m2026-01-10T15:55:53Z\u001b[0m \u001b[32mINF\u001b[0m Retrying connection in up to 1s \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.7\n",
            "\u001b[90m2026-01-10T15:55:53Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Connection terminated \u001b[36mconnIndex=\u001b[0m0\n",
            "\u001b[90m2026-01-10T15:55:53Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m no more connections active and exiting\n",
            "\u001b[90m2026-01-10T15:55:53Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel server stopped\n",
            "\u001b[90m2026-01-10T15:55:53Z\u001b[0m \u001b[32mINF\u001b[0m Metrics server stopped\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}