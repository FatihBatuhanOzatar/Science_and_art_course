{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VS-GzRHfjHgP",
        "outputId": "f777abe9-5ac3-4653-8e40-fbded5cf5771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kâ ‡\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ‹\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K\n",
            "changed 22 packages in 2s\n",
            "\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub streamlit -q\n",
        "!npm install -g localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "HF_TOKEN = \"xxxxxxxxxxxxxxxxxxxx\" #Token removed for security\n",
        "\n",
        "st.set_page_config(page_title=\"GenAI Cloud\", layout=\"wide\")\n",
        "st.title(\"ðŸš€ Science & Art: GenAI (Cloud Version)\")\n",
        "st.success(\"Sistem BaÄŸlandÄ±! / System Connected!\")\n",
        "\n",
        "if not HF_TOKEN.startswith(\"hf_\"):\n",
        "    st.error(\"Token hatasÄ±!\")\n",
        "    st.stop()\n",
        "\n",
        "client = InferenceClient(token=HF_TOKEN)\n",
        "\n",
        "mode = st.sidebar.radio(\"Select Mode:\", [\"ðŸ’¬ Chat Mode\", \"ðŸŽ¨ Art Mode\"])\n",
        "\n",
        "# --- CHAT MODE ---\n",
        "if mode == \"ðŸ’¬ Chat Mode\":\n",
        "    st.header(\"ðŸ’¬ Chatbot (Mistral 7B)\")\n",
        "    if \"messages\" not in st.session_state: st.session_state.messages = []\n",
        "\n",
        "    for msg in st.session_state.messages:\n",
        "        st.chat_message(msg[\"role\"]).markdown(msg[\"content\"])\n",
        "\n",
        "    if prompt := st.chat_input(\"Ask something...\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        st.chat_message(\"user\").markdown(prompt)\n",
        "\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Thinking...\"):\n",
        "                try:\n",
        "                    response = \"\"\n",
        "                    for token in client.chat_completion([{\"role\": \"user\", \"content\": prompt}], model=\"mistralai/Mistral-7B-Instruct-v0.2\", max_tokens=500, stream=True):\n",
        "                        if token.choices[0].delta.content:\n",
        "                            response += token.choices[0].delta.content\n",
        "                    st.markdown(response)\n",
        "                    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Hata: {e}\")\n",
        "\n",
        "# --- ART MODE ---\n",
        "elif mode == \"ðŸŽ¨ Art Mode\":\n",
        "    st.header(\"ðŸŽ¨ Art Generator (SDXL)\")\n",
        "    prompt = st.text_input(\"Describe image:\", \"A futuristic city\")\n",
        "    if st.button(\"Generate Image\"):\n",
        "        with st.spinner(\"Generating...\"):\n",
        "            try:\n",
        "                image = client.text_to_image(prompt, model=\"stabilityai/stable-diffusion-xl-base-1.0\")\n",
        "                st.image(image, caption=prompt)\n",
        "            except Exception as e:\n",
        "                st.error(f\"Hata: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6oKFyPbjWTR",
        "outputId": "607e5cc2-7f6d-4680-fdfc-170727ba5090"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb > cloudflared.deb\n",
        "!dpkg -i cloudflared.deb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utPRrvevjX1T",
        "outputId": "8a50d1d3-9acf-4f22-e431-5d4e2b156e9f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 121693 files and directories currently installed.)\n",
            "Preparing to unpack cloudflared.deb ...\n",
            "Unpacking cloudflared (2025.11.1) over (2025.11.1) ...\n",
            "Setting up cloudflared (2025.11.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & cloudflared tunnel --url http://localhost:8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPx4jaRujbLY",
        "outputId": "695ed13e-1e2a-418c-aecb-0ae05546edbc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2026-01-10T16:06:40Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2026-01-10T16:06:40Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.126.126.243:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[90m2026-01-10T16:06:43Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2026-01-10T16:06:43Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2026-01-10T16:06:43Z\u001b[0m \u001b[32mINF\u001b[0m |  https://surrey-cartoons-cliff-championship.trycloudflare.com                              |\n",
            "\u001b[90m2026-01-10T16:06:43Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2026-01-10T16:06:43Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2026-01-10T16:06:43Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.11.1 (Checksum 83ea55259e419549817460d0c097f23ad1327364d0a63fab2c5463b9283251cb)\n",
            "\u001b[90m2026-01-10T16:06:43Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.9, GoArch: amd64\n",
            "\u001b[90m2026-01-10T16:06:43Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2026-01-10T16:06:43Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update if installed by a package manager.\n",
            "\u001b[90m2026-01-10T16:06:43Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 9d1a0d6b-889c-46c3-89ab-899a6b6b4718\n",
            "\u001b[90m2026-01-10T16:06:43Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2026-01-10T16:06:43Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2026-01-10T16:06:43Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2026-01-10T16:06:43Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Cannot determine default origin certificate path. No file cert.pem in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]. You need to specify the origin certificate path by specifying the origincert option in the configuration file, or set TUNNEL_ORIGIN_CERT environment variable \u001b[36moriginCertPath=\u001b[0m\n",
            "\u001b[90m2026-01-10T16:06:43Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2026-01-10T16:06:43Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2026-01-10T16:06:43Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2026-01-10T16:06:43Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.13\n",
            "2026/01/10 16:06:43 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2026-01-10T16:06:44Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m22f7cfb4-36c1-4241-a744-4a844fa20aa1 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.13 \u001b[36mlocation=\u001b[0msin14 \u001b[36mprotocol=\u001b[0mquic\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[90m2026-01-10T16:46:57Z\u001b[0m \u001b[32mINF\u001b[0m Initiating graceful shutdown due to signal interrupt ...\n",
            "\u001b[90m2026-01-10T16:46:57Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to run the datagram handler \u001b[31merror=\u001b[0m\u001b[31m\"context canceled\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.13\n",
            "\u001b[90m2026-01-10T16:46:57Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to serve tunnel connection \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.13\n",
            "\u001b[90m2026-01-10T16:46:57Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Serve tunnel error \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.13\n",
            "\u001b[90m2026-01-10T16:46:57Z\u001b[0m \u001b[32mINF\u001b[0m Retrying connection in up to 1s \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.13\n",
            "\u001b[90m2026-01-10T16:46:57Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Connection terminated \u001b[36mconnIndex=\u001b[0m0\n",
            "\u001b[90m2026-01-10T16:46:57Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m no more connections active and exiting\n",
            "\u001b[90m2026-01-10T16:46:57Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel server stopped\n",
            "\u001b[90m2026-01-10T16:46:57Z\u001b[0m \u001b[32mINF\u001b[0m Metrics server stopped\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}